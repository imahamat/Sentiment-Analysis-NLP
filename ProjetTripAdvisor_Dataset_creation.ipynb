{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjetTripAdvisor_Partie_1_Dataset .ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-essa/Sentiment-Analysis-and-Satisfaction-Prediction/blob/master/ProjetTripAdvisor_Dataset_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHCmWiLOFrd",
        "colab_type": "code",
        "outputId": "65b8cd0f-3451-477d-c14e-b2a65dc91cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hljNrxUVO6Pz",
        "colab_type": "code",
        "outputId": "6e8b550d-c651-4b50-eae6-e2ca7e79439d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras.preprocessing.text\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix as cm\n",
        "from sklearn.metrics import multilabel_confusion_matrix as mcm\n",
        "import tensorflow_hub as hub\n",
        "tf.keras.backend.clear_session()\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "pd.options.display.max_colwidth = 1000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbkBdgm3O8Uo",
        "colab_type": "code",
        "outputId": "e51922a6-8f3a-49fb-8df6-79ac1316ef01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9gorZ5KO-GN",
        "colab_type": "code",
        "outputId": "eae9e4de-7f4d-42c8-e706-2f358504451e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17Acb55O_xN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "df=pd.read_csv('/content/gdrive/My Drive/hotel_reviews.csv', sep=';', header=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUruoMgPT-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del df['Unnamed: 0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvE4Kd2vPWJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(dataframe):\n",
        "  msk = np.random.rand(len(dataframe)) < 0.8\n",
        "\n",
        "  train = dataframe[msk]\n",
        "\n",
        "  test = dataframe[~msk]\n",
        "  #print(train_file_path.head)\n",
        "  all_data = np.array(dataframe.values.tolist())\n",
        "  training_set = np.array(train.values.tolist())\n",
        "  test_set=np.array(test.values.tolist())\n",
        "  print(\"Dataset Length: \",len(training_set)+ len(test_set))\n",
        "  return all_data, training_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoq0nxa5Pis2",
        "colab_type": "code",
        "outputId": "d84caa1b-fb2e-4721-c645-5226c51547b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "all_data, training_set, test_set =create_dataset(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Length:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcvMVP7ymnV",
        "colab_type": "code",
        "outputId": "958cb40a-d743-4abb-a90f-5a07fb0f9898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "def clean_set(set_len):\n",
        "  stop_words = list(stopwords.words('english'))\n",
        "  stop_words.remove(\"not\")\n",
        "  stop_words.remove(\"no\")\n",
        "  stop_words.remove(\"isn't\")\n",
        "  stop_words.remove(\"hasn't\")\n",
        "  stop_words.remove(\"wasn't\")\n",
        "  stop_words.remove(\"didn't\")\n",
        "  stop_words.remove(\"haven't\")\n",
        "  stop_words.remove(\"don't\")\n",
        "  stop_words.remove(\"doesn't\")\n",
        "  #stop_words.remove(\"didn't\")\n",
        "  stop_words.remove(\"weren't\")\n",
        "  stop_words.remove(\"shouldn't\")\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  for elem in set_len:\n",
        "    word_tokens = word_tokenize(elem[1]) \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(lemmatizer.lemmatize(w)) \n",
        "    elem[1]=' '.join(filtered_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8IZh6ISi76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_set(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag3_rQbaQFs0",
        "colab_type": "text"
      },
      "source": [
        "#IMDB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7svGjl2vP27P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2gbQUkiQc_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_train_examples = []\n",
        "for i in train_examples :\n",
        "  label_train_examples.append(i.decode(\"utf-8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7CCsvNCQfH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_test_examples = []\n",
        "for i in test_examples :\n",
        "  label_test_examples.append(i.decode(\"utf-8\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lqRqsEKQg1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_data = {'label':train_labels ,\n",
        "                       'text':label_train_examples\n",
        "                       }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Fsjn3hQioN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_examples_data = {'label':test_labels ,\n",
        "                       'text':label_test_examples\n",
        "                       }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeIgF9uCQkyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_data_tfds = pd.DataFrame(train_examples_data)\n",
        "test_examples_data_tfds = pd.DataFrame(test_examples_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ozd0I-8QmpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_data_tfds = np.array(train_examples_data_tfds.values.tolist())\n",
        "test_examples_data_tfds=np.array(test_examples_data_tfds.values.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41rEeaRWQqox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_set(train_examples_data_tfds)\n",
        "clean_set(test_examples_data_tfds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiRwSa_rtD-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data_tfds = np.append(train_examples_data_tfds, test_examples_data_tfds, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQI1601RX_CB",
        "colab_type": "text"
      },
      "source": [
        "#Somme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka1KX4tiQvTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data_tripAdvisor_tfds= np.append(all_data_tfds, all_data, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPWydv9zxDKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data_tripAdvisor_tfds_DataFrame = pd.DataFrame(all_data_tripAdvisor_tfds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGMNq6caEsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_TripAdvisor_DataFrame = pd.DataFrame(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxngba3NaEll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_TripAdvisor_DataFrame = pd.DataFrame(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXWL1uPyaEdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_data_IMDB_DataFrame = pd.DataFrame(train_examples_data_tfds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX180JTAakvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_examples_data_IMDB_DataFrame = pd.DataFrame(test_examples_data_tfds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66u9sKlaoe0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data_tripAdvisor_tfds_DataFrame.to_csv(\"Total_data_for_tokenizer.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ9ApCN7bDJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set_TripAdvisor_DataFrame.to_csv(\"training_set_TripAdvisor.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslbrNISbDCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_TripAdvisor_DataFrame.to_csv(\"test_set_TripAdvisor.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKhDIDYWbC6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_data_IMDB_DataFrame.to_csv(\"training_set_IMDB.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJGLFQoybCyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_examples_data_IMDB_DataFrame.to_csv(\"test_set_IMDB.csv\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d01lYddJr2Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuu8Xb1sr2DD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/Total_data_for_tokenizer.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoe7Vd69bmq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/training_set_TripAdvisor.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiFTxwYGbmij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/test_set_TripAdvisor.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13deCR_ibmYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/training_set_IMDB.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyG4uEtQbmJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/test_set_IMDB.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}